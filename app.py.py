# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sG15uCVqb6dF2GRzQRK6xPb8RYpzuUwt
"""

#es
!pip install transformers datasets

# Install necessary librari
# Import required libraries
import pandas as pd
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
from datasets import Dataset
import torch

# Load the CSV data (upload the CSV first in Colab or use it from your Google Drive)
from google.colab import files
uploaded = files.upload()

# Read CSV file (replace 'data_infinity.csv' with the uploaded filename)
df = pd.read_csv('/content/data_infinity.csv')



# Combine 'Input' and 'Response' into a single text (format: Question followed by Answer)
df['text'] = df['Input'] + " " + df['Response']

# Convert the DataFrame to Hugging Face Dataset format
dataset = Dataset.from_pandas(df[['text']])

# Load the GPT-2 tokenizer and model
model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Add padding token to the tokenizer (set pad_token as eos_token)
tokenizer.pad_token = tokenizer.eos_token

# Tokenize the dataset and add 'labels'
def tokenize_function(examples):
    # Tokenize inputs and set the labels to be identical to the inputs
    tokenized_output = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)
    tokenized_output['labels'] = tokenized_output['input_ids'].copy()  # Set input_ids as labels for language modeling
    return tokenized_output

# Apply the tokenization
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Set format for PyTorch (required for training)
tokenized_datasets.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

# Split the dataset into training and testing datasets (90% train, 10% test)
train_test_split = tokenized_datasets.train_test_split(test_size=0.1)
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',          # Output directory for checkpoints and model
    num_train_epochs=3,              # Number of epochs
    per_device_train_batch_size=2,   # Batch size for training
    per_device_eval_batch_size=2,    # Batch size for evaluation
    warmup_steps=100,                # Number of warmup steps
    weight_decay=0.01,               # Strength of weight decay
    logging_dir='./logs',            # Directory for storing logs
    logging_steps=10,
    save_total_limit=2               # Limit the total amount of checkpoints
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset
)

# Fine-tune the GPT-2 model
trainer.train()

# Save the trained model
model.save_pretrained('fine_tuned_gpt2_infinity')
tokenizer.save_pretrained('fine_tuned_gpt2_infinity')

# Zip the model directory for download
!zip -r fine_tuned_gpt2_infinity.zip fine_tuned_gpt2_infinity

# Provide a download link
from google.colab import files
files.download('fine_tuned_gpt2_infinity.zip')

